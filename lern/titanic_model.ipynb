{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas #ipython notebook\n",
    "titanic = pandas.read_csv(\"titanic_train.csv\")\n",
    "titanic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.361582    0.523008   \n",
      "std     257.353842    0.486592    0.836071   13.019697    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   35.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "print(titanic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "print(titanic[\"Sex\"].unique())\n",
    "\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "print(titanic[\"Embarked\"].unique())\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna('S')\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_test = pandas.read_csv(\"titanic_test.csv\")\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=10, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "target='Survived'# Survived的值就是分类的输出  \n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "X = titanic[predictors]# 分割 x 向量变量\n",
    "y = titanic[target]# 目标训练字段\n",
    "gbdt = GradientBoostingClassifier(random_state=10)#分类模型\n",
    "gbdt.fit(X,y)#模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8889\n",
      "AUC Score (Train): 0.946583\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.96      0.91       549\n",
      "          1       0.92      0.78      0.84       342\n",
      "\n",
      "avg / total       0.89      0.89      0.89       891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics  \n",
    "y_pred = gbdt.predict(X)# 预测\n",
    "y_predprob = gbdt.predict_proba(X)[:,1] # 预测概率\n",
    "print (\"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred))\n",
    "print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob))\n",
    "print(metrics.classification_report(y.values, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用测试集进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0453087 ,  0.31492968,  0.2271579 ,  0.11208302,  0.36061187,\n",
       "        0.1050144 ,  0.45814015,  0.18052626,  0.88709337,  0.08657651,\n",
       "        0.08225089,  0.15499689,  0.94357795,  0.25032299,  0.90897853,\n",
       "        0.93400018,  0.08487987,  0.17675905,  0.5280773 ,  0.52461293,\n",
       "        0.27856014,  0.64440851,  0.94724671,  0.48356076,  0.96206427,\n",
       "        0.05806317,  0.97135221,  0.17675905,  0.54486608,  0.15912965,\n",
       "        0.06465929,  0.15110409,  0.45525861,  0.21734469,  0.6697365 ,\n",
       "        0.15468511,  0.41079323,  0.38708377,  0.10070989,  0.49177213,\n",
       "        0.06762399,  0.52791144,  0.06768424,  0.88584518,  0.9270119 ,\n",
       "        0.14188046,  0.1233614 ,  0.13733638,  0.95666762,  0.58546077,\n",
       "        0.31655857,  0.16210628,  0.86166477,  0.85749495,  0.19073429,\n",
       "        0.03459709,  0.07058627,  0.09937164,  0.10284057,  0.97260726,\n",
       "        0.08753385,  0.1917797 ,  0.09690674,  0.83424506,  0.6254223 ,\n",
       "        0.8817299 ,  0.74347597,  0.15328593,  0.27730358,  0.91857177,\n",
       "        0.76446539,  0.08387871,  0.48635183,  0.39813039,  0.97260726,\n",
       "        0.28904654,  0.11002736,  0.8671482 ,  0.11935492,  0.76446539,\n",
       "        0.88891892,  0.0422663 ,  0.37248311,  0.08225089,  0.15317797,\n",
       "        0.13202448,  0.78533676,  0.41171229,  0.74776238,  0.94409222,\n",
       "        0.36061187,  0.09111458,  0.92893923,  0.11002736,  0.43287075,\n",
       "        0.09290007,  0.93401262,  0.14717866,  0.47849869,  0.14420466,\n",
       "        0.95248732,  0.10498814,  0.13733638,  0.10349274,  0.62312501,\n",
       "        0.11510767,  0.15673065,  0.13733638,  0.10465818,  0.11149373,\n",
       "        0.05196694,  0.74776238,  0.96715685,  0.84515286,  0.94662422,\n",
       "        0.12945617,  0.04914639,  0.88985953,  0.50908791,  0.87806614,\n",
       "        0.94860183,  0.13379647,  0.975425  ,  0.08525487,  0.13733638,\n",
       "        0.52065044,  0.09290007,  0.70877046,  0.11051866,  0.10590047,\n",
       "        0.19006623,  0.11246658,  0.16918545,  0.04775378,  0.067197  ,\n",
       "        0.08387871,  0.2195033 ,  0.11894052,  0.40046143,  0.0855531 ,\n",
       "        0.0946234 ,  0.95433623,  0.08301835,  0.11510767,  0.53627361,\n",
       "        0.3507512 ,  0.25063754,  0.10590047,  0.52791144,  0.10535974,\n",
       "        0.97135221,  0.14581644,  0.03612164,  0.47920799,  0.0652197 ,\n",
       "        0.09937164,  0.94194769,  0.46687115,  0.53627361,  0.43223773,\n",
       "        0.76128421,  0.8140202 ,  0.87970543,  0.0428109 ,  0.11051866,\n",
       "        0.43372862,  0.43049507,  0.14778153,  0.93748946,  0.38708377,\n",
       "        0.07621023,  0.2195033 ,  0.09091745,  0.13696893,  0.23708648,\n",
       "        0.93475709,  0.92592532,  0.39397171,  0.94499196,  0.90691469,\n",
       "        0.11935492,  0.47091537,  0.94636923,  0.13733638,  0.95411589,\n",
       "        0.11411721,  0.8710938 ,  0.11007669,  0.07003098,  0.11051866,\n",
       "        0.12895915,  0.27925809,  0.33457304,  0.02589795,  0.81782802,\n",
       "        0.07058627,  0.95589381,  0.50649723,  0.10070989,  0.41079323,\n",
       "        0.75980614,  0.96196853,  0.16150289,  0.9480264 ,  0.10070989,\n",
       "        0.33319142,  0.52023882,  0.10070989,  0.87791209,  0.08387871,\n",
       "        0.1980199 ,  0.0428109 ,  0.26157204,  0.71277543,  0.79107541,\n",
       "        0.26975703,  0.74776238,  0.09132867,  0.95852471,  0.11002736,\n",
       "        0.82432341,  0.10590047,  0.85848744,  0.09290007,  0.8002554 ,\n",
       "        0.55953605,  0.09290007,  0.74776238,  0.0981643 ,  0.14027708,\n",
       "        0.38204391,  0.95818163,  0.08337936,  0.13733638,  0.45662207,\n",
       "        0.08753385,  0.12855812,  0.17675905,  0.85021811,  0.96620857,\n",
       "        0.77515887,  0.91469678,  0.59030099,  0.08225089,  0.09798143,\n",
       "        0.23246326,  0.8710938 ,  0.15973005,  0.87806614,  0.50045667,\n",
       "        0.85733892,  0.14765574,  0.27287288,  0.10070989,  0.1549787 ,\n",
       "        0.07621023,  0.13733638,  0.11002736,  0.87175566,  0.09290007,\n",
       "        0.1022805 ,  0.08387871,  0.89608875,  0.84380536,  0.07767569,\n",
       "        0.08225089,  0.08433898,  0.07621023,  0.41079323,  0.1050144 ,\n",
       "        0.29169803,  0.13733638,  0.95943324,  0.75424987,  0.13696893,\n",
       "        0.87243304,  0.10465818,  0.09250535,  0.09240404,  0.10070989,\n",
       "        0.38708377,  0.96410213,  0.74776238,  0.44811296,  0.77741513,\n",
       "        0.0633341 ,  0.08525487,  0.31655857,  0.13696893,  0.11002736,\n",
       "        0.25063754,  0.08839606,  0.13696893,  0.29565412,  0.13237424,\n",
       "        0.09354882,  0.91269401,  0.15912965,  0.26611987,  0.08225089,\n",
       "        0.11487167,  0.13552484,  0.06172426,  0.10070989,  0.74776238,\n",
       "        0.79648016,  0.14193646,  0.88877144,  0.17688706,  0.15764423,\n",
       "        0.1050144 ,  0.17675905,  0.07621023,  0.50897393,  0.97185228,\n",
       "        0.8066378 ,  0.21215471,  0.1050144 ,  0.09354882,  0.15110409,\n",
       "        0.10349274,  0.17675905,  0.11894052,  0.6038244 ,  0.95695812,\n",
       "        0.06752141,  0.93610548,  0.29169803,  0.09798143,  0.10695537,\n",
       "        0.94108454,  0.19576088,  0.13696893,  0.77543446,  0.09354882,\n",
       "        0.29581046,  0.1917797 ,  0.03568899,  0.19722926,  0.13696893,\n",
       "        0.1050144 ,  0.09649376,  0.15255708,  0.97570873,  0.08378754,\n",
       "        0.60301337,  0.11894052,  0.65503566,  0.10695537,  0.93399845,\n",
       "        0.95216774,  0.10070989,  0.14142647,  0.21660659,  0.84574408,\n",
       "        0.20128678,  0.93008187,  0.08225089,  0.13733638,  0.48314815,\n",
       "        0.08939268,  0.95059586,  0.93399845,  0.11208302,  0.97154744,\n",
       "        0.23601693,  0.13202448,  0.22905764,  0.95216774,  0.37953188,\n",
       "        0.09091745,  0.98410133,  0.09918646,  0.10868018,  0.93347079,\n",
       "        0.9550425 ,  0.32447   ,  0.10695537,  0.12942865,  0.32289357,\n",
       "        0.13733638,  0.17321166,  0.31159668,  0.47561677,  0.11111826,\n",
       "        0.9340091 ,  0.09290007,  0.04102288,  0.15673065,  0.05524708,\n",
       "        0.36099823,  0.93751512,  0.44120307,  0.08804855,  0.03659827,\n",
       "        0.94601083,  0.16683097,  0.97479198,  0.09290007,  0.06581322,\n",
       "        0.94621226,  0.08536962,  0.97135221,  0.25135114,  0.12213388,\n",
       "        0.19221172,  0.08551879,  0.20925463,  0.7240909 ,  0.81664931,\n",
       "        0.74776238,  0.97326206,  0.50554842,  0.11002736,  0.96984656,\n",
       "        0.06322654,  0.11002736,  0.1850629 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt = titanic_test[predictors]# 分割 x 向量变量\n",
    "y_pred = gbdt.predict(Xt)# 预测\n",
    "y_predprob = gbdt.predict_proba(Xt)[:,1] # 预测概率\n",
    "y_predprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型调参 (加分项)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "首先我们从步长(learning rate)和迭代次数(n_estimators)入手。一般来说,开始选择一个较小的步长来网格搜索最好的迭代次数。这里，我们将步长初始值设置为0.1。对于迭代次数进行网格搜索如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.85553, std: 0.01667, params: {'n_estimators': 20},\n",
       "  mean: 0.86189, std: 0.02003, params: {'n_estimators': 30},\n",
       "  mean: 0.86272, std: 0.02391, params: {'n_estimators': 40},\n",
       "  mean: 0.86358, std: 0.02543, params: {'n_estimators': 50},\n",
       "  mean: 0.86602, std: 0.02466, params: {'n_estimators': 60},\n",
       "  mean: 0.86667, std: 0.02410, params: {'n_estimators': 70},\n",
       "  mean: 0.86516, std: 0.02558, params: {'n_estimators': 80}],\n",
       " {'n_estimators': 70},\n",
       " 0.8666668213138685)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param_test1 = {'n_estimators':[i for i in range(20,81,10)]}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=300,\n",
    "                                  min_samples_leaf=20,max_depth=8,max_features='sqrt', subsample=0.8,random_state=10), \n",
    "                       param_grid = param_test1, scoring='roc_auc',iid=False,cv=5)\n",
    "gsearch1.fit(X,y)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到了一个合适的迭代次数，现在我们开始对决策树进行调参。首先我们对决策树最大深度max_depth和内部节点再划分所需最小样本数min_samples_split进行网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.87117, std: 0.02827, params: {'max_depth': 3, 'min_samples_split': 100},\n",
       "  mean: 0.87104, std: 0.02773, params: {'max_depth': 3, 'min_samples_split': 200},\n",
       "  mean: 0.86467, std: 0.02377, params: {'max_depth': 3, 'min_samples_split': 300},\n",
       "  mean: 0.86369, std: 0.01934, params: {'max_depth': 3, 'min_samples_split': 400},\n",
       "  mean: 0.87008, std: 0.02834, params: {'max_depth': 5, 'min_samples_split': 100},\n",
       "  mean: 0.87101, std: 0.02950, params: {'max_depth': 5, 'min_samples_split': 200},\n",
       "  mean: 0.86639, std: 0.02496, params: {'max_depth': 5, 'min_samples_split': 300},\n",
       "  mean: 0.86447, std: 0.02042, params: {'max_depth': 5, 'min_samples_split': 400},\n",
       "  mean: 0.87075, std: 0.02974, params: {'max_depth': 7, 'min_samples_split': 100},\n",
       "  mean: 0.86961, std: 0.03127, params: {'max_depth': 7, 'min_samples_split': 200},\n",
       "  mean: 0.86667, std: 0.02410, params: {'max_depth': 7, 'min_samples_split': 300},\n",
       "  mean: 0.86375, std: 0.02138, params: {'max_depth': 7, 'min_samples_split': 400},\n",
       "  mean: 0.87315, std: 0.02883, params: {'max_depth': 9, 'min_samples_split': 100},\n",
       "  mean: 0.86798, std: 0.03210, params: {'max_depth': 9, 'min_samples_split': 200},\n",
       "  mean: 0.86667, std: 0.02410, params: {'max_depth': 9, 'min_samples_split': 300},\n",
       "  mean: 0.86375, std: 0.02138, params: {'max_depth': 9, 'min_samples_split': 400},\n",
       "  mean: 0.87164, std: 0.02866, params: {'max_depth': 11, 'min_samples_split': 100},\n",
       "  mean: 0.86771, std: 0.03172, params: {'max_depth': 11, 'min_samples_split': 200},\n",
       "  mean: 0.86667, std: 0.02410, params: {'max_depth': 11, 'min_samples_split': 300},\n",
       "  mean: 0.86375, std: 0.02138, params: {'max_depth': 11, 'min_samples_split': 400},\n",
       "  mean: 0.87233, std: 0.02765, params: {'max_depth': 13, 'min_samples_split': 100},\n",
       "  mean: 0.86771, std: 0.03172, params: {'max_depth': 13, 'min_samples_split': 200},\n",
       "  mean: 0.86667, std: 0.02410, params: {'max_depth': 13, 'min_samples_split': 300},\n",
       "  mean: 0.86375, std: 0.02138, params: {'max_depth': 13, 'min_samples_split': 400}],\n",
       " {'max_depth': 9, 'min_samples_split': 100},\n",
       " 0.8731491609589407)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {'max_depth':[i for i in range(3,14,2)], 'min_samples_split':[i for i in range(100,401,100)]}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=70, min_samples_leaf=20, \n",
    "      max_features='sqrt', subsample=0.8, random_state=10), \n",
    "   param_grid = param_test2, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch2.fit(X,y)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把它定下来，对于内部节点再划分所需最小样本数min_samples_split，我们暂时不能一起定下来，因为这个还和决策树其他的参数存在关联。下面我们再对内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf一起调参。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.86629, std: 0.03300, params: {'min_samples_leaf': 10, 'min_samples_split': 10},\n",
       "  mean: 0.86629, std: 0.03300, params: {'min_samples_leaf': 10, 'min_samples_split': 20},\n",
       "  mean: 0.86756, std: 0.02948, params: {'min_samples_leaf': 10, 'min_samples_split': 30},\n",
       "  mean: 0.86940, std: 0.03045, params: {'min_samples_leaf': 10, 'min_samples_split': 40},\n",
       "  mean: 0.86566, std: 0.03050, params: {'min_samples_leaf': 10, 'min_samples_split': 50},\n",
       "  mean: 0.87183, std: 0.03025, params: {'min_samples_leaf': 10, 'min_samples_split': 60},\n",
       "  mean: 0.86999, std: 0.03293, params: {'min_samples_leaf': 10, 'min_samples_split': 70},\n",
       "  mean: 0.86993, std: 0.03069, params: {'min_samples_leaf': 10, 'min_samples_split': 80},\n",
       "  mean: 0.86992, std: 0.02969, params: {'min_samples_leaf': 10, 'min_samples_split': 90},\n",
       "  mean: 0.86900, std: 0.03124, params: {'min_samples_leaf': 10, 'min_samples_split': 100},\n",
       "  mean: 0.87251, std: 0.03051, params: {'min_samples_leaf': 20, 'min_samples_split': 10},\n",
       "  mean: 0.87251, std: 0.03051, params: {'min_samples_leaf': 20, 'min_samples_split': 20},\n",
       "  mean: 0.87251, std: 0.03051, params: {'min_samples_leaf': 20, 'min_samples_split': 30},\n",
       "  mean: 0.87251, std: 0.03051, params: {'min_samples_leaf': 20, 'min_samples_split': 40},\n",
       "  mean: 0.87440, std: 0.02789, params: {'min_samples_leaf': 20, 'min_samples_split': 50},\n",
       "  mean: 0.87318, std: 0.02984, params: {'min_samples_leaf': 20, 'min_samples_split': 60},\n",
       "  mean: 0.87547, std: 0.02975, params: {'min_samples_leaf': 20, 'min_samples_split': 70},\n",
       "  mean: 0.87316, std: 0.02624, params: {'min_samples_leaf': 20, 'min_samples_split': 80},\n",
       "  mean: 0.87364, std: 0.02745, params: {'min_samples_leaf': 20, 'min_samples_split': 90},\n",
       "  mean: 0.87315, std: 0.02883, params: {'min_samples_leaf': 20, 'min_samples_split': 100},\n",
       "  mean: 0.86938, std: 0.02981, params: {'min_samples_leaf': 30, 'min_samples_split': 10},\n",
       "  mean: 0.86938, std: 0.02981, params: {'min_samples_leaf': 30, 'min_samples_split': 20},\n",
       "  mean: 0.86938, std: 0.02981, params: {'min_samples_leaf': 30, 'min_samples_split': 30},\n",
       "  mean: 0.86938, std: 0.02981, params: {'min_samples_leaf': 30, 'min_samples_split': 40},\n",
       "  mean: 0.86938, std: 0.02981, params: {'min_samples_leaf': 30, 'min_samples_split': 50},\n",
       "  mean: 0.86938, std: 0.02981, params: {'min_samples_leaf': 30, 'min_samples_split': 60},\n",
       "  mean: 0.87082, std: 0.03055, params: {'min_samples_leaf': 30, 'min_samples_split': 70},\n",
       "  mean: 0.87102, std: 0.02930, params: {'min_samples_leaf': 30, 'min_samples_split': 80},\n",
       "  mean: 0.87189, std: 0.02686, params: {'min_samples_leaf': 30, 'min_samples_split': 90},\n",
       "  mean: 0.86980, std: 0.03047, params: {'min_samples_leaf': 30, 'min_samples_split': 100},\n",
       "  mean: 0.87096, std: 0.02442, params: {'min_samples_leaf': 40, 'min_samples_split': 10},\n",
       "  mean: 0.87096, std: 0.02442, params: {'min_samples_leaf': 40, 'min_samples_split': 20},\n",
       "  mean: 0.87096, std: 0.02442, params: {'min_samples_leaf': 40, 'min_samples_split': 30},\n",
       "  mean: 0.87096, std: 0.02442, params: {'min_samples_leaf': 40, 'min_samples_split': 40},\n",
       "  mean: 0.87096, std: 0.02442, params: {'min_samples_leaf': 40, 'min_samples_split': 50},\n",
       "  mean: 0.87096, std: 0.02442, params: {'min_samples_leaf': 40, 'min_samples_split': 60},\n",
       "  mean: 0.87096, std: 0.02442, params: {'min_samples_leaf': 40, 'min_samples_split': 70},\n",
       "  mean: 0.87096, std: 0.02442, params: {'min_samples_leaf': 40, 'min_samples_split': 80},\n",
       "  mean: 0.86768, std: 0.02526, params: {'min_samples_leaf': 40, 'min_samples_split': 90},\n",
       "  mean: 0.86626, std: 0.02623, params: {'min_samples_leaf': 40, 'min_samples_split': 100},\n",
       "  mean: 0.86228, std: 0.02762, params: {'min_samples_leaf': 50, 'min_samples_split': 10},\n",
       "  mean: 0.86228, std: 0.02762, params: {'min_samples_leaf': 50, 'min_samples_split': 20},\n",
       "  mean: 0.86228, std: 0.02762, params: {'min_samples_leaf': 50, 'min_samples_split': 30},\n",
       "  mean: 0.86228, std: 0.02762, params: {'min_samples_leaf': 50, 'min_samples_split': 40},\n",
       "  mean: 0.86228, std: 0.02762, params: {'min_samples_leaf': 50, 'min_samples_split': 50},\n",
       "  mean: 0.86228, std: 0.02762, params: {'min_samples_leaf': 50, 'min_samples_split': 60},\n",
       "  mean: 0.86228, std: 0.02762, params: {'min_samples_leaf': 50, 'min_samples_split': 70},\n",
       "  mean: 0.86228, std: 0.02762, params: {'min_samples_leaf': 50, 'min_samples_split': 80},\n",
       "  mean: 0.86228, std: 0.02762, params: {'min_samples_leaf': 50, 'min_samples_split': 90},\n",
       "  mean: 0.86228, std: 0.02762, params: {'min_samples_leaf': 50, 'min_samples_split': 100},\n",
       "  mean: 0.85862, std: 0.02330, params: {'min_samples_leaf': 60, 'min_samples_split': 10},\n",
       "  mean: 0.85862, std: 0.02330, params: {'min_samples_leaf': 60, 'min_samples_split': 20},\n",
       "  mean: 0.85862, std: 0.02330, params: {'min_samples_leaf': 60, 'min_samples_split': 30},\n",
       "  mean: 0.85862, std: 0.02330, params: {'min_samples_leaf': 60, 'min_samples_split': 40},\n",
       "  mean: 0.85862, std: 0.02330, params: {'min_samples_leaf': 60, 'min_samples_split': 50},\n",
       "  mean: 0.85862, std: 0.02330, params: {'min_samples_leaf': 60, 'min_samples_split': 60},\n",
       "  mean: 0.85862, std: 0.02330, params: {'min_samples_leaf': 60, 'min_samples_split': 70},\n",
       "  mean: 0.85862, std: 0.02330, params: {'min_samples_leaf': 60, 'min_samples_split': 80},\n",
       "  mean: 0.85862, std: 0.02330, params: {'min_samples_leaf': 60, 'min_samples_split': 90},\n",
       "  mean: 0.85862, std: 0.02330, params: {'min_samples_leaf': 60, 'min_samples_split': 100},\n",
       "  mean: 0.85638, std: 0.02108, params: {'min_samples_leaf': 70, 'min_samples_split': 10},\n",
       "  mean: 0.85638, std: 0.02108, params: {'min_samples_leaf': 70, 'min_samples_split': 20},\n",
       "  mean: 0.85638, std: 0.02108, params: {'min_samples_leaf': 70, 'min_samples_split': 30},\n",
       "  mean: 0.85638, std: 0.02108, params: {'min_samples_leaf': 70, 'min_samples_split': 40},\n",
       "  mean: 0.85638, std: 0.02108, params: {'min_samples_leaf': 70, 'min_samples_split': 50},\n",
       "  mean: 0.85638, std: 0.02108, params: {'min_samples_leaf': 70, 'min_samples_split': 60},\n",
       "  mean: 0.85638, std: 0.02108, params: {'min_samples_leaf': 70, 'min_samples_split': 70},\n",
       "  mean: 0.85638, std: 0.02108, params: {'min_samples_leaf': 70, 'min_samples_split': 80},\n",
       "  mean: 0.85638, std: 0.02108, params: {'min_samples_leaf': 70, 'min_samples_split': 90},\n",
       "  mean: 0.85638, std: 0.02108, params: {'min_samples_leaf': 70, 'min_samples_split': 100},\n",
       "  mean: 0.85375, std: 0.02078, params: {'min_samples_leaf': 80, 'min_samples_split': 10},\n",
       "  mean: 0.85375, std: 0.02078, params: {'min_samples_leaf': 80, 'min_samples_split': 20},\n",
       "  mean: 0.85375, std: 0.02078, params: {'min_samples_leaf': 80, 'min_samples_split': 30},\n",
       "  mean: 0.85375, std: 0.02078, params: {'min_samples_leaf': 80, 'min_samples_split': 40},\n",
       "  mean: 0.85375, std: 0.02078, params: {'min_samples_leaf': 80, 'min_samples_split': 50},\n",
       "  mean: 0.85375, std: 0.02078, params: {'min_samples_leaf': 80, 'min_samples_split': 60},\n",
       "  mean: 0.85375, std: 0.02078, params: {'min_samples_leaf': 80, 'min_samples_split': 70},\n",
       "  mean: 0.85375, std: 0.02078, params: {'min_samples_leaf': 80, 'min_samples_split': 80},\n",
       "  mean: 0.85375, std: 0.02078, params: {'min_samples_leaf': 80, 'min_samples_split': 90},\n",
       "  mean: 0.85375, std: 0.02078, params: {'min_samples_leaf': 80, 'min_samples_split': 100},\n",
       "  mean: 0.85119, std: 0.01924, params: {'min_samples_leaf': 90, 'min_samples_split': 10},\n",
       "  mean: 0.85119, std: 0.01924, params: {'min_samples_leaf': 90, 'min_samples_split': 20},\n",
       "  mean: 0.85119, std: 0.01924, params: {'min_samples_leaf': 90, 'min_samples_split': 30},\n",
       "  mean: 0.85119, std: 0.01924, params: {'min_samples_leaf': 90, 'min_samples_split': 40},\n",
       "  mean: 0.85119, std: 0.01924, params: {'min_samples_leaf': 90, 'min_samples_split': 50},\n",
       "  mean: 0.85119, std: 0.01924, params: {'min_samples_leaf': 90, 'min_samples_split': 60},\n",
       "  mean: 0.85119, std: 0.01924, params: {'min_samples_leaf': 90, 'min_samples_split': 70},\n",
       "  mean: 0.85119, std: 0.01924, params: {'min_samples_leaf': 90, 'min_samples_split': 80},\n",
       "  mean: 0.85119, std: 0.01924, params: {'min_samples_leaf': 90, 'min_samples_split': 90},\n",
       "  mean: 0.85119, std: 0.01924, params: {'min_samples_leaf': 90, 'min_samples_split': 100},\n",
       "  mean: 0.84741, std: 0.01728, params: {'min_samples_leaf': 100, 'min_samples_split': 10},\n",
       "  mean: 0.84741, std: 0.01728, params: {'min_samples_leaf': 100, 'min_samples_split': 20},\n",
       "  mean: 0.84741, std: 0.01728, params: {'min_samples_leaf': 100, 'min_samples_split': 30},\n",
       "  mean: 0.84741, std: 0.01728, params: {'min_samples_leaf': 100, 'min_samples_split': 40},\n",
       "  mean: 0.84741, std: 0.01728, params: {'min_samples_leaf': 100, 'min_samples_split': 50},\n",
       "  mean: 0.84741, std: 0.01728, params: {'min_samples_leaf': 100, 'min_samples_split': 60},\n",
       "  mean: 0.84741, std: 0.01728, params: {'min_samples_leaf': 100, 'min_samples_split': 70},\n",
       "  mean: 0.84741, std: 0.01728, params: {'min_samples_leaf': 100, 'min_samples_split': 80},\n",
       "  mean: 0.84741, std: 0.01728, params: {'min_samples_leaf': 100, 'min_samples_split': 90},\n",
       "  mean: 0.84741, std: 0.01728, params: {'min_samples_leaf': 100, 'min_samples_split': 100}],\n",
       " {'min_samples_leaf': 20, 'min_samples_split': 70},\n",
       " 0.8754747793522167)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {'min_samples_split':[ i for i in range(10,101,10)], 'min_samples_leaf':[ i for i in range(10,101,10)]}\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=70,max_depth=9,\n",
    "                                     max_features='sqrt', subsample=0.8, random_state=10), \n",
    "                       param_grid = param_test3, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch3.fit(X,y)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们再对最大特征数max_features进行网格搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.87178, std: 0.02983, params: {'max_features': 1},\n",
       "  mean: 0.87547, std: 0.02975, params: {'max_features': 2},\n",
       "  mean: 0.87612, std: 0.02722, params: {'max_features': 3},\n",
       "  mean: 0.87102, std: 0.02612, params: {'max_features': 4},\n",
       "  mean: 0.87321, std: 0.02942, params: {'max_features': 5},\n",
       "  mean: 0.87623, std: 0.02815, params: {'max_features': 6}],\n",
       " {'max_features': 6},\n",
       " 0.876230350028832)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4 = {'max_features':[ i for i in range(1,7,1)]}\n",
    "gsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=70,max_depth=9, min_samples_leaf =20, \n",
    "               min_samples_split =70, subsample=0.8, random_state=10), \n",
    "                       param_grid = param_test4, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch4.fit(X,y)\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们再对子采样的比例进行网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.87283, std: 0.02361, params: {'subsample': 0.6},\n",
       "  mean: 0.87336, std: 0.02541, params: {'subsample': 0.7},\n",
       "  mean: 0.87425, std: 0.02541, params: {'subsample': 0.75},\n",
       "  mean: 0.87623, std: 0.02815, params: {'subsample': 0.8},\n",
       "  mean: 0.87507, std: 0.02509, params: {'subsample': 0.85},\n",
       "  mean: 0.87183, std: 0.02644, params: {'subsample': 0.9}],\n",
       " {'subsample': 0.8},\n",
       " 0.876230350028832)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n",
    "gsearch5 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=70,max_depth=9, min_samples_leaf =20, \n",
    "               min_samples_split =70, max_features=6, random_state=10), \n",
    "                       param_grid = param_test5, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch5.fit(X,y)\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们基本已经得到我们所有调优的参数结果了。这时我们可以减半步长，最大迭代次数加倍来增加我们模型的泛化能力。再次拟合我们的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9001\n",
      "AUC Score (Train): 0.961285\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.95      0.92       549\n",
      "          1       0.91      0.82      0.86       342\n",
      "\n",
      "avg / total       0.90      0.90      0.90       891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbdt = GradientBoostingClassifier(learning_rate=0.1, n_estimators=70,max_depth=9, min_samples_leaf =20, \n",
    "               min_samples_split =70, max_features=6, subsample=0.8, random_state=10)\n",
    "gbdt.fit(X,y)\n",
    "y_pred = gbdt.predict(X)\n",
    "y_predprob = gbdt.predict_proba(X)[:,1]\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred))\n",
    "print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob))\n",
    "print(metrics.classification_report(y.values, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
